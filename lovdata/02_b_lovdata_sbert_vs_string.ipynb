{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"annotations.csv\", sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have four annotated columns, each corresponding to an investigative question\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Dokument\")\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find how many indexes has a string-value in a column\n",
    "def has_value(row):\n",
    "    # check if there's any string in this row:\n",
    "    return any([isinstance(row[col], str) for col in cols])\n",
    "\n",
    "df[\"has_value\"] = df.apply(has_value, axis=1)\n",
    "df.head()\n",
    "\n",
    "# count the number of \"has_value\":\n",
    "df[\"has_value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from ColumnName -> IBn\n",
    "col_to_ib = {}\n",
    "ibs = [\"IB1\", \"IB2\", \"IB3\", \"IB4\"]\n",
    "for ib, col in zip(ibs, cols):\n",
    "    col_to_ib[col] = ib\n",
    "ib_to_col = {v: k for k, v in col_to_ib.items()}\n",
    "\n",
    "to_dict = {}\n",
    "\n",
    "for i in df.index:\n",
    "    to_dict[i] = {}\n",
    "    for col in cols:\n",
    "        if not pd.isna(df.loc[i, col]):\n",
    "            _id = col_to_ib[col]\n",
    "            to_dict[i][_id] = df.loc[i, col]\n",
    "\n",
    "# remove empty\n",
    "to_dict = {k: v for k, v in to_dict.items() if v}\n",
    "to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {col: col_to_ib[col] for col in cols}\n",
    "df = df.rename(columns=rename_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_statistics(colname):\n",
    "    col = df[colname]\n",
    "    print(\"Column: {}\".format(colname))\n",
    "    print(\"Number of unique values: {}\".format(len(col.unique())))\n",
    "\n",
    "for col in ib_to_col.keys():\n",
    "    column_statistics(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib1 = \"Tilknytning til åstedet eller gjerningsadressen\"\n",
    "ib2 = \"Død, dødsårsak og skader\"\n",
    "ib3 = \"Fornærmede involvert i konflikt\"\n",
    "ib4 = \"Våpen, drapsvåpen og våpenbruk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk sentence tokenizer for norwegian\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentencize(text):\n",
    "    return sent_tokenize(text, language='norwegian')\n",
    "\n",
    "sentencize(df.iloc[1][\"Dokument\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a df with sentences, with the paragraph id\n",
    "\n",
    "# 1. add paragraph id\n",
    "df[\"Paragraph\"] = df.index\n",
    "# 2. sentencize, 1 row for each sentence\n",
    "df_sentences = df.apply(lambda row: pd.Series(sentencize(row[\"Dokument\"])), axis=1).stack().reset_index(level=1, drop=True).to_frame(\"Sentence\")\n",
    "# 3. add back paragraph id\n",
    "df_sentences = df_sentences.join(df[\"Paragraph\"], how=\"left\")\n",
    "\n",
    "# also add a column for each IB, IB1, IB2, IB3, IB4\n",
    "for ib in ibs:\n",
    "    df_sentences[ib] = None\n",
    "df_sentences.head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter sentences less than 10 characters long\n",
    "df_sentences = df_sentences[df_sentences[\"Sentence\"].str.len() > 10]\n",
    "df_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, check if there's a match in any of the\n",
    "# indexes at the paragraph index of the original df\n",
    "\n",
    "i = 0\n",
    "\n",
    "paragraphs_with_matches = set()\n",
    "\n",
    "sentence_data = {}\n",
    "\n",
    "for row_idx, row in enumerate(df_sentences.iterrows()):\n",
    "    para_id = row[1][\"Paragraph\"]\n",
    "    sent = row[1][\"Sentence\"]\n",
    "    # print(f\"Row {row_idx} - Paragraph {para_id} - Sentence: {sent}\")\n",
    "\n",
    "    sentence_data[row_idx] = {\n",
    "        \"Paragraph\": para_id,\n",
    "        \"Sent\": sent,\n",
    "        \"IB1\": False,\n",
    "        \"IB2\": False,\n",
    "        \"IB3\": False,\n",
    "        \"IB4\": False,\n",
    "    }\n",
    "\n",
    "    if para_id not in to_dict:\n",
    "        continue\n",
    "\n",
    "    # then, find matching IBs for this paragraph ID, and check each sentence!\n",
    "    for ib, text in to_dict[para_id].items():\n",
    "        # the text may be split into several sentences, as one IB can match multiple!\n",
    "        candidate_sents = text.split(\";\")\n",
    "        for cand_sent in candidate_sents:\n",
    "            for cs in sentencize(cand_sent):\n",
    "                cs = cs.strip()\n",
    "                if cs.lower() in sent.lower():\n",
    "                    paragraphs_with_matches.add(para_id)\n",
    "                    sentence_data[row_idx][ib] = True\n",
    "len(paragraphs_with_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.DataFrame.from_dict(sentence_data, orient=\"index\")\n",
    "sent_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_with_matches = sorted(list(paragraphs_with_matches))\n",
    "print(paragraphs_with_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sentences that has any of IB1, IB2, IB3, IB4 as True\n",
    "sentences_with_matches = sent_df[sent_df[ibs].any(axis=1)]\n",
    "sentences_with_matches = sentences_with_matches.index.tolist()\n",
    "print(sentences_with_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def preprocess(document, stop=True, stem=True, lower=True):\n",
    "    tokens = nltk.word_tokenize(document, language='norwegian')\n",
    "    if stop:\n",
    "        stop_words = nltk.corpus.stopwords.words('norwegian')\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    if stem:\n",
    "        stemmer = nltk.stem.snowball.NorwegianStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    # remove tokens with less than 3 chars and only keep alpha\n",
    "    tokens = [t.lower() for t in tokens if len(t) >= 2 and t.isalpha()]\n",
    "    # return \" \".join(tokens).strip()\n",
    "    return tokens\n",
    "\n",
    "print(preprocess(df.iloc[0][\"Dokument\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1) + len(set2) - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def filter_results(scores, threshold=0, top_n=20):\n",
    "    results = [(i, score) for i, score in enumerate(scores) if score > threshold]\n",
    "    results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    if top_n == 0:\n",
    "        return results\n",
    "    return results[:top_n]\n",
    "\n",
    "def print_results(results, docs, sent_df):\n",
    "    for i, score in results:\n",
    "        print(f\"--- {sent_df.iloc[i]['Sent']}\")\n",
    "        print(f\"------ Score: {score} - {docs[i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_simple_metrics(query, docs):\n",
    "    # dct = Dictionary(docs)\n",
    "    # corpus = [dct.doc2bow(doc) for doc in docs]\n",
    "    # query = dct.doc2bow(preprocess(query))\n",
    "    all_scores = {}\n",
    "    q = preprocess(query)\n",
    "\n",
    "    # simple string search\n",
    "    # initialize an empty array of th elength of docs\n",
    "    string_sim = np.zeros(len(docs))\n",
    "    for q_token in q:\n",
    "        # check all documents if they contain this q_token\n",
    "        for i, doc in enumerate(docs):\n",
    "            if q_token in doc:\n",
    "                string_sim[i] += 1\n",
    "    all_scores[\"string\"] = string_sim\n",
    "\n",
    "\n",
    "    # tf-idf + cosine\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=lambda doc: doc,\n",
    "        preprocessor=lambda doc: doc,\n",
    "        ngram_range=(1, 3),\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "    cosine_sim = cosine_similarity(vectorizer.transform([q]), tfidf_matrix).flatten()\n",
    "    all_scores[\"tfidf\"] = cosine_sim\n",
    "\n",
    "    # jaccard\n",
    "    j_query = set(q)\n",
    "    j_sims = [jaccard_similarity(j_query, set(j_doc)) for j_doc in docs]\n",
    "    # j_sims = (j_sims - np.min(j_sims)) / (np.max(j_sims) - np.min(j_sims))\n",
    "    all_scores[\"jaccard\"] = j_sims\n",
    "\n",
    "    # bm25\n",
    "    bm25 = BM25Okapi(docs)\n",
    "    bm25_scores = bm25.get_scores(q)\n",
    "    # bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
    "    all_scores[\"bm25\"] = bm25_scores\n",
    "\n",
    "    # normalize all scores:\n",
    "    for k, v in all_scores.items():\n",
    "        if np.max(v) != np.min(v):\n",
    "            all_scores[k] = (v - np.min(v)) / (np.max(v) - np.min(v))\n",
    "        else:\n",
    "            all_scores[k] = np.zeros(len(v))\n",
    "\n",
    "    return all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "\n",
    "fasttext.util.download_model('no', if_exists='ignore')  # English\n",
    "fasttext_model = fasttext.load_model('cc.no.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sbert_model = SentenceTransformer(\"NbAiLab/nb-sbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute fasttext sentence vectors for all sentences in the corpus\n",
    "sent_df[\"fasttext\"] = sent_df[\"Sent\"].apply(lambda x: fasttext_model.get_sentence_vector(x))\n",
    "sent_df[\"sbert\"] = sent_df[\"Sent\"].apply(lambda x: sbert_model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, euclidean, cityblock, minkowski\n",
    "\n",
    "def compute_similarity_with_scipy(query_sentence, dataframe, column=\"fasttext\", top_n=20):\n",
    "    vectors = dataframe[column].values\n",
    "    sentence_indexes = dataframe.index.values\n",
    "\n",
    "    metric_mapping = {\n",
    "        'cosine': cosine,\n",
    "        'euclidean': euclidean,\n",
    "        'manhattan': cityblock,\n",
    "        'minkowski': lambda u, v: minkowski(u, v, p=2)\n",
    "    }\n",
    "\n",
    "    top_similar_matches = {metric: [] for metric in metric_mapping}\n",
    "\n",
    "    for metric, distance_func in metric_mapping.items():\n",
    "        distances = [distance_func(query_sentence, vector) for vector in vectors]\n",
    "\n",
    "        if metric == 'cosine':\n",
    "            similarity_scores = [1 - distance for distance in distances]\n",
    "        else:\n",
    "            max_distance = max(distances)\n",
    "            similarity_scores = [(max_distance - distance) / max_distance for distance in distances]\n",
    "\n",
    "        top_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:top_n]\n",
    "        top_similar_matches[metric] = [(similarity_scores[i], sentence_indexes[i]) for i in top_indices]\n",
    "\n",
    "    return top_similar_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_index(ib=\"IB1\", verbose=True):\n",
    "    ib_df = sent_df[sent_df[ib] == True]\n",
    "    ib_indexes = set(ib_df.index.values)\n",
    "    ib_paragraphs = set(ib_df[\"Paragraph\"].values)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"True sentences:\")\n",
    "        for i in sorted(ib_indexes):\n",
    "            print(f\"-- IDX {i} - {sent_df.iloc[i]['Sent']}\")\n",
    "\n",
    "        print(\"True paragraphs:\")\n",
    "        print(sorted(ib_paragraphs))\n",
    "    return ib_indexes, ib_paragraphs\n",
    "\n",
    "def get_sentids(filtered_scores):\n",
    "    return [sent_id for sent_id, _ in filtered_scores]\n",
    "\n",
    "docs = [preprocess(sent) for sent in sent_df[\"Sent\"].tolist()]\n",
    "def build_results(query, TOP_N):\n",
    "    all_scores = get_simple_metrics(query, docs)\n",
    "    index_results = {}\n",
    "\n",
    "    for similarity_type in all_scores.keys():\n",
    "        # return all (top_n=0)\n",
    "        sim_indexes = get_sentids(filter_results(all_scores[similarity_type], top_n=TOP_N))\n",
    "        index_results[similarity_type] = sim_indexes\n",
    "\n",
    "    fast_query = fasttext_model.get_sentence_vector(query)\n",
    "    fast_sim = compute_similarity_with_scipy(fast_query, sent_df, column=\"fasttext\", top_n=TOP_N)\n",
    "    index_results[\"fasttext\"] = [i for _, i in fast_sim[\"cosine\"]]\n",
    "    # for metric, res in fast_sim.items():\n",
    "    #     metric_name = f\"fasttext_{metric}\"\n",
    "    #     index_results[metric_name] = [i for _, i in res]\n",
    "\n",
    "    sbert_query = sbert_model.encode(query)\n",
    "    sbert_sim = compute_similarity_with_scipy(sbert_query, sent_df, column=\"sbert\", top_n=TOP_N)\n",
    "    index_results[\"sbert\"] = [i for _, i in sbert_sim[\"cosine\"]]\n",
    "    # for metric, res in sbert_sim.items():\n",
    "    #     metric_name = f\"sbert_{metric}\"\n",
    "    #     index_results[metric_name] = [i for _, i in res]\n",
    "\n",
    "    return index_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib1_indexes, ib1_paragraphs = get_true_index(\"IB1\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def compare_with_index(query, sent_df, ib_indexes, ib_paragraphs, print_sents=False, print_diff=False):\n",
    "    if print_sents:\n",
    "        print(f\"Searching with '{query}'\\n\")\n",
    "    index_results = build_results(query, len(ib_indexes))\n",
    "    # index_results = build_results(query, TOP_N=15)\n",
    "    # iterate indexes and show sentences in sent_df:\n",
    "    final_matching_paragraphs = {}\n",
    "\n",
    "    for metric, indexes in index_results.items():\n",
    "        matching_indexes = set(indexes).intersection(ib_indexes)\n",
    "        paragraphs = set(sent_df[sent_df.index.isin(matching_indexes)][\"Paragraph\"].values)\n",
    "        matching_paragraphs = paragraphs.intersection(ib_paragraphs)\n",
    "        percentage_paras = len(matching_paragraphs) / len(ib_paragraphs)\n",
    "        \n",
    "        if print_sents:\n",
    "            print(f\"Results for {metric}: {sorted(indexes)}\")\n",
    "            print(f\"Matching indexes: {sorted(matching_indexes)}\")\n",
    "            print(f\"Matching paragraphs: {sorted(matching_paragraphs)} ({percentage_paras:.2%})\")\n",
    "            for i in indexes:\n",
    "                print(f\"-- {sent_df.iloc[i]['Sent']}\")\n",
    "            print(\"-\"*40)\n",
    "\n",
    "        if print_diff:\n",
    "            non_matching_indexes = set(indexes).difference(ib_indexes)\n",
    "            print(f\"Non-matching indexes: {sorted(non_matching_indexes)}\")\n",
    "            for i in non_matching_indexes:\n",
    "                print(f\"-- {sent_df.iloc[i]['Sent']}\")\n",
    "\n",
    "        final_matching_paragraphs[metric] = {}\n",
    "        final_matching_paragraphs[metric][\"matching_sentences\"] = matching_indexes\n",
    "        final_matching_paragraphs[metric][\"matching_paragraphs\"] = matching_paragraphs\n",
    "\n",
    "        sentence_accuracy = round(len(matching_indexes) / len(ib_indexes), 2)\n",
    "        final_matching_paragraphs[metric][\"sent_accuracy\"] = sentence_accuracy\n",
    "\n",
    "        para_accuracy = round(len(matching_paragraphs) / len(ib_paragraphs), 2)\n",
    "        final_matching_paragraphs[metric][\"para_accuracy\"] = para_accuracy\n",
    "\n",
    "        final_matching_paragraphs[metric][\"model_sentences\"] = indexes\n",
    "        final_matching_paragraphs[metric][\"model_paragraphs\"] = paragraphs\n",
    "\n",
    "    return final_matching_paragraphs\n",
    "\n",
    "# Hvem har tilknytning til åstedet / gjerningsadressen?\n",
    "# q = \"person i, rundt, eller i nærheten av huset\"\n",
    "# q = \"person i eller rundt hus eller adressen\"\n",
    "q = \"person med tilknytning til boligen / adressen\"\n",
    "matches = compare_with_index(q, sent_df, ib1_indexes, ib1_paragraphs)\n",
    "matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib2_indexes, ib2_paragraphs = get_true_index(\"IB2\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hva har skjedd - dødsårsak\n",
    "q = \"årsak til dødsfallet\"\n",
    "compare_with_index(q, sent_df, ib2_indexes, ib2_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib3_indexes, ib3_paragraphs = get_true_index(\"IB3\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var fornærmede involvert i konflikt?\n",
    "q = \"fornærmede / avdøde involvert i konflikt\" \n",
    "compare_with_index(q, sent_df, ib3_indexes, ib3_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib4_indexes, ib4_paragraphs = get_true_index(\"IB4\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drapsvåpenet, hva vet vi om drapsvåpenet?\n",
    "q = \"informasjon om våpenet\"\n",
    "compare_with_index(q, sent_df, ib4_indexes, ib4_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"inneholder en rekke rom som på gjerningstidspunktet ble utleid til arbeidstakere fra forskjellige østeuropeiske land\"\n",
    "def find_index(sent, sent_df):\n",
    "    return sent_df[sent_df[\"Sent\"].str.contains(sent)].index.values[0]\n",
    "\n",
    "def find_index(sent, sent_df):\n",
    "    return sent_df[sent_df[\"Sent\"].str.contains(sent, regex=False)].index.values[0]\n",
    "\n",
    "find_index(s, sent_df)\n",
    "idx = find_index(\"som er angitt i tiltalen leide C husrom gjennom F i [adresse]\", sent_df)\n",
    "sent_df.iloc[idx][\"Sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the paragraphs of any sentence index\n",
    "# e.g. 9 --> paragraph containing the sentence index\n",
    "def find_paragraphs(sent_index, sent_df):\n",
    "    return sent_df[sent_df.index == sent_index][\"Paragraph\"].values[0]\n",
    "assert find_paragraphs(0, sent_df) == 0\n",
    "assert find_paragraphs(5, sent_df) == 1\n",
    "assert find_paragraphs(6, sent_df) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {\n",
    "    \"sbert\": {\n",
    "        \"IB1\": \"person med tilknytning til boligen / adressen\", \n",
    "        \"IB2\": \"årsak til dødsfallet\", \n",
    "        \"IB3\": \"fornærmede / avdøde involvert i konflikt\",\n",
    "        \"IB4\": \"informasjon om våpenet\", \n",
    "    },\n",
    "    \"string\": {\n",
    "        \"IB1\": \"boligen\",\n",
    "        \"IB2\": \"dødsårsak\",\n",
    "        \"IB3\": \"konflikt\",\n",
    "        \"IB4\": \"drapsvåpen\",\n",
    "    }\n",
    "}\n",
    "print(queries.keys())\n",
    "\n",
    "\n",
    "total_retrieved = {}\n",
    "USE_INVESTIGATOR = False\n",
    "\n",
    "for ib in [\"IB1\", \"IB2\", \"IB3\", \"IB4\"]:\n",
    "    print(ib)\n",
    "    model_results_all = build_results(q, TOP_N=100)\n",
    "\n",
    "    # model_results = sorted(model_results[\"sbert\"])\n",
    "    for model_type in queries.keys():\n",
    "        model_results = model_results_all[model_type]\n",
    "        _id = f\"{ib}_{model_type}\"\n",
    "        q = queries[model_type][ib]\n",
    "\n",
    "        total_retrieved[_id] = []\n",
    "\n",
    "        indexes, paragraphs = get_true_index(ib, verbose=False)\n",
    "        print(sorted(indexes))\n",
    "        print(sorted(paragraphs))\n",
    "\n",
    "        if USE_INVESTIGATOR:\n",
    "            for paragraph in sorted(paragraphs):\n",
    "                tmp = {\n",
    "                    \"type\": None,  # etterforsker, modell, begge\n",
    "                    \"paragraf\": paragraph,\n",
    "                    \"setning\": None,\n",
    "                    \"tekst\": None,\n",
    "                }\n",
    "\n",
    "                # find the corresponding investigator sentence\n",
    "                investigator_sent = df.iloc[paragraph][ib]\n",
    "                if investigator_sent:\n",
    "                    investigator_sent = [s.strip() for s in investigator_sent.split(\";\")]\n",
    "                else:\n",
    "                    investigator_sent = []\n",
    "\n",
    "                for inv_sents in investigator_sent:\n",
    "                    # there are possibly multiple sentences in one:\n",
    "                    for inv_sent in sentencize(inv_sents):\n",
    "                        _tmp = tmp.copy()\n",
    "                        inv_index = find_index(inv_sent, sent_df)\n",
    "                        _tmp[\"setning\"] = inv_index\n",
    "                        _tmp[\"tekst\"] = sent_df.iloc[inv_index][\"Sent\"]\n",
    "                        if inv_index in model_results:\n",
    "                            _tmp[\"type\"] = \"begge\"\n",
    "                        else:\n",
    "                            _tmp[\"type\"] = \"etterforsker\"\n",
    "                        total_retrieved[_id].append(_tmp)\n",
    "\n",
    "        for model_sent in model_results:\n",
    "            added = False\n",
    "            # if model_sent is already in total_retrieved, change its type to \"begge\"\n",
    "            for x in total_retrieved[_id]:\n",
    "                if x[\"setning\"] == model_sent:\n",
    "                    x[\"type\"] = \"begge\"\n",
    "                    added = True\n",
    "            if not added:\n",
    "                total_retrieved[_id].append({\n",
    "                    \"type\": \"modell\",\n",
    "                    \"query\": q,\n",
    "                    \"paragraf\": find_paragraphs(model_sent, sent_df),\n",
    "                    \"setning\": model_sent,\n",
    "                    \"tekst\": sent_df.iloc[model_sent][\"Sent\"],\n",
    "                })\n",
    "\n",
    "\n",
    "        # sort by paragraph -> setning\n",
    "        total_retrieved[_id] = sorted(total_retrieved[_id], key=lambda x: (x[\"paragraf\"], x[\"setning\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ib, results in total_retrieved.items():\n",
    "    print(ib)\n",
    "    for res in results:\n",
    "        print(res)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ib, results in total_retrieved.items():\n",
    "    for res in results:\n",
    "        res[\"IB\"] = ib\n",
    "\n",
    "res_df = pd.DataFrame([res for results in total_retrieved.values() for res in results])\n",
    "res_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some stats...\n",
    "\n",
    "# how many sentences are retrieved by each model?\n",
    "res_df.groupby(\"IB\")[\"setning\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(\"fritekst_vs_sbert.csv\", index=False, columns=[\"IB\", \"type\", \"paragraf\", \"setning\", \"tekst\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
